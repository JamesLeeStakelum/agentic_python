
# Python LLM Project Architecture & Design Patterns

This document outlines the core Python functions and standard design patterns to be used in all Large Language Model (LLM) integration projects. Adherence to these guidelines is **critical** for consistency, maintainability, and reliable operation.

**It's imperative that the LLM uses the specified functions and design patterns exclusively.** **Don't generate alternative implementations or deviate from these established patterns.**

---

## 1. Core Python Utility Functions

This section details the foundational Python functions comprising the utility library. The LLM **must use these functions** when performing tasks that fall within their scope.

### 1.1 File I/O and System Utilities

* **`read_file(file_path: str) -> str | None`**
    * **Purpose**: Reads the content of any text file, intelligently determining its encoding.
    * **Key Features**:
        * Robustly handles a wide range of encodings (UTF-8, ISO-8859-1, Windows-1252, UTF-16LE/BE, etc.) using `chardet` for detection.
        * Automatically removes Byte Order Marks (BOMs).
        * Normalizes all line endings to Unix-style (`\n`).
    * **Returns**: The file content as a string, an empty string (`""`) if the file is empty, or `None` on failure (e.g., file not found).

* **`write_file(file_path: str, content: str) -> bool`**
    * **Purpose**: Writes provided content to a specified file.
    * **Key Features**:
        * Ensures the output file is written with **UTF-8 encoding**.
        * Converts Windows-style newlines (`\r\n`) to Unix-style (`\n`) for consistency.
    * **Returns**: `True` on success, `False` on failure.

* **`generate_random_string(length: int = 20) -> str`**
    * **Purpose**: Generates a random alphanumeric string.
    * **Parameters**: `length` (optional, defaults to `20` characters).
    * **Returns**: A random string of the specified length.

* **`trim(input_string: str) -> str`**
    * **Purpose**: Removes leading and trailing whitespace from a string.
    * **Returns**: The trimmed string.

* **`ensure_directory(directory_path: str | Path) -> None`**
    * **Purpose**: Ensures that a given directory path exists.
    * **Key Features**: Creates any intermediate directories as needed.
    * **Dependency**: Internally uses `pathlib.Path`.

* **`create_temp_folder(folder_path: str = "./temp") -> None`**
    * **Purpose**: Ensures that the `./temp` directory exists. This is specifically for temporary file storage related to LLM calls.

* **`clear_temp_folder(threshold: float = 3600.0, folder: str | Path = "./temp") -> None`**
    * **Purpose**: Deletes files from the `./temp` folder that are older than a specified time threshold.
    * **Parameters**:
        * `threshold` (float, optional): Age cutoff in seconds. Defaults to `3600.0` (1 hour).
        * `folder` (str or Path, optional): The directory to clean. Defaults to `./temp`.

### 1.2 LLM Interaction Functions

* **`call_llm($prompt, $template, $config_file, $logs_folder)`**
    * **Note**: This function is part of the Perl library. The Python environment will require an equivalent function to interact with the `call_openrouter.exe` utility. The conceptual workflow remains the same.
    * **Purpose**: The primary function for communicating with the external LLM command-line utility (`call_openrouter.exe`).
    * **Workflow**:
        1.  Generates unique temporary input/output file names in the `./temp` directory using `generate_random_string()`.
        2.  Writes the provided `prompt` (UTF-8 encoded) to the temporary input file using `write_file()`.
        3.  Constructs and executes a system command to invoke `call_openrouter.exe`. This executable handles connecting to the LLM (using configuration data like API keys from `config_file`), sending the prompt, and writing the response to the output file.
        4.  Reads the LLM's response from the temporary output file using `read_file()`.
        5.  Trims leading/trailing whitespace from the response using `trim()`.
    * **Returns**: The LLM's response as a string, or an empty string (`""`) on error.

* **`hill_climbing(folder: str, candidate_prompt: str, judge_count: int, max_iterations: int, evaluation_criteria_file: str = "") -> None`**
    * **Purpose**: Implements an iterative improvement algorithm for LLM outputs, allowing for reflection and refinement. This function is used when the LLM response needs to be iteratively improved based on feedback.
    * **Workflow**:
        1.  Generates an initial candidate solution using the `candidate_prompt` via `call_llm()`.
        2.  Enters a loop (up to `max_iterations` times) for refinement:
            * Retrieves the current "best" solution.
            * **Critiques** the current "best" by generating a special prompt that asks an LLM for advice on how to improve it, considering the original prompt and `evaluation_criteria`.
            * Generates a **new candidate** solution using `candidate_prompt`, incorporating the advice received from the critique step.
            * Compares the new candidate against the current "best" using the `judge_voting()` helper function.
            * If the new candidate is deemed superior (by majority vote of the "judges"), it replaces the "best" solution.
    * **Parameters**:
        * `folder` (str): The directory to store `best.txt` and `candidate.txt` files, which track the progress of the hill climbing.
        * `candidate_prompt` (str): The base prompt used to generate candidate solutions.
        * `judge_count` (int): The number of "judges" (separate LLM calls) used within `judge_voting()` for comparison.
        * `max_iterations` (int): The maximum number of refinement iterations.
        * `evaluation_criteria_file` (str, optional): A file containing the specific criteria for evaluating the quality/accuracy of candidate solutions.
    * **Returns**: Void (the best solution is written to `f"{folder}/best.txt"`).

* **`judge_voting(best_version: str, new_candidate: str, evaluation_criteria: str, judge_count: int, original_prompt: str) -> str`**
    * **Purpose**: A **helper function for `hill_climbing()`**. It compares two LLM-generated versions (a "best" and a "candidate") based on specified criteria, using multiple LLM calls as "judges" to ensure objective evaluation.
    * **Workflow**:
        1.  Constructs a prompt for an LLM "judge," clearly presenting both the `best_version` and `new_candidate`, the `original_prompt` context, and the `evaluation_criteria`.
        2.  Executes `call_llm()` `judge_count` times, each time asking a "judge" to select the better version (indicated by returning `'1'` or `'2'`).
        3.  Tallies the votes from all judges.
    * **Returns**: `'1'` if the first version wins the majority vote, `'2'` if the second version wins.

### 1.3 Text Cleaning Function

* **`remove_non_ascii(text: str) -> str`**
    * **Purpose**: Cleans text by removing or normalizing specific non-ASCII characters that LLMs frequently generate, ensuring output cleanliness and compatibility.
    * **Key Features**:
        * **Removes emoticons**.
        * **Normalizes curly quotes (`“”‘’«»`) and curly apostrophes to straight quotes (`"`, `'`)**.
        * Normalizes em-dashes (`—`) and en-dashes (`–`) to double hyphens (`--`).
        * Normalizes ellipsis (`…`) to three periods (`...`).
        * Removes zero-width characters and other Unicode "control" or "format" characters.
        * **Crucially**: Preserves accented characters, non-Latin scripts, and common typographical/currency symbols.
        * Normalizes all line endings to Unix-style (`\n`).
    * **Returns**: The cleaned string.

### 1.4 Tag Extraction Function

* **`extract_text_between_tags(text: str, tag: str, strict: bool = False) -> str`**
    * **Purpose**: Extracts content enclosed within specific tags from an LLM response. This function is **designed to be highly robust** against common LLM formatting inconsistencies and missing tags.
    * **Key Features**:
        * **Flexible Tag Matching**: Handles variations in tag syntax (e.g., `< answer >`, `< /answer >`, `<ANSWER>`) and common misspellings (`<answe?r?>`, `<answers?>`).
        * **Intelligent Boundary Handling**: Can infer a missing closing tag (e.g., `</answer>`) if it's followed by known semantic boundary tags like `<comments>` or `<thinking>`.
        * **Optional Strict Mode**: If `strict=True` is passed, it requires both a perfectly matched opening and closing tag for extraction; otherwise, it operates in a more flexible mode.
        * **Post-Extraction Cleanup**: Trims leading/trailing whitespace from the extracted content.
    * **Parameters**:
        * `text` (str): The full LLM response string.
        * `tag` (str): The name of the tag to extract (e.g., `'answer'`, `'comments'`, `'thinking'`). Case-insensitivity is handled internally.
        * `strict` (bool, optional): If `True`, requires both opening and closing tags. Defaults to `False`.
    * **Returns**: The extracted content as a string, or an empty string (`""`) if content for the specified tag cannot be robustly extracted.

### 1.5 Data Management and Transfer Solutions

* **SQLite for Persistent/Complex Data**:
    * **Purpose**: Use **SQLite databases** for lightweight, portable, and persistent data storage in Python scripts. This is especially valuable for data that needs to survive script execution, for complex data structures (beyond simple dictionaries), and for both text and binary data.
    * **Advantages**: Free, single-file, easy to use, highly portable (works across Windows, macOS, Linux), can be in-memory or disk-based. Provides excellent traceability and visibility into data after script execution. Easily handles complex data structures. Supports both text and binary data.
    * **Implementation Guidance**:
        * Your code should **automatically create the database file, tables, and indexes at startup**, using `CREATE TABLE IF NOT EXISTS` and `CREATE INDEX IF NOT EXISTS` statements to ensure idempotency.
        * **Leverage SQL for complex data structures** that would be cumbersome with Python dictionaries.

* **ChromaDB for Vector Embeddings**:
    * **Purpose**: For managing and querying **vector embeddings**, use **ChromaDB**.
    * **Advantages**: Free, lightweight, and easy to use for vector search and similarity.
    * **Implementation Guidance**: Integrate ChromaDB specifically for any vector-related data storage and retrieval tasks.

* **Base64 Encoding for Binary Data Transfer (HTTPS/HTTP)**:
    * **Purpose**: When sending **binary data** (e.g., images, files, raw bytes) over HTTP/HTTPS, **ALWAYS encode it with Base64**.
    * **Reasoning**: HTTP/HTTPS protocols (and underlying network infrastructure) work best with text-based data. Sending raw binary data can lead to corruption or misinterpretation. Base64 converts binary into a safe, text-only representation.
    * **Implementation Guidance**: Before sending binary data in an HTTP request body or as a parameter, convert it to a Base64 string. On the receiving end, decode the Base64 string back into its original binary form.

---

## 2. LLM Design Patterns

These are standard design patterns that **must be applied** in Python LLM projects to ensure structured, predictable, and easily parsable outputs.

### 2.1 Response Tagging (Core Structural Pattern)

* **Intent**: To achieve clear and reliable separation between the LLM's primary answer and any other accompanying content. This is fundamental for automated parsing.
* **Mandatory Tags**:
    * The primary answer **MUST ALWAYS** be enclosed within **`<answer>...</answer>`** tags.
    * Any comments, meta-information, or LLM internal thoughts **MUST ALWAYS** be placed within **`<comments>...</comments>`** tags.
* **When to Use**: **Always**, for any LLM prompt where you need to extract the core response reliably.
* **Function Used**: `extract_text_between_tags()` is specifically designed to work with this pattern.

### 2.2 Comments Section (Core Structural Pattern)

* **Intent**: To explicitly separate explanatory comments, thoughts, or rationale from the main answer.
* **Implementation**: Instruct the LLM to use the `<comments>...</comments>` tags as described in the "Response Tagging" pattern.
* **When to Use**: When the LLM might provide useful meta-commentary or its thinking process.

### 2.3 Multiple Response Sections (Core Structural Pattern)

* **Intent**: To capture multiple, distinct structured fields within a single LLM response.
* **Mechanism**: Define unique tag pairs for each desired section (e.g., `<title>...</title>`, `<summary>...</summary>`, `<outline>...</outline>`).
* **When to Use**: When a prompt elicits several different pieces of information that need to be extracted individually.
* **Function Used**: `extract_text_between_tags()` can be called multiple times, once for each expected tag.

### 2.4 Extracting Tagged Content (Core Structural Pattern)

* **Intent**: To enable robust and automated parsing of specific fields from an LLM's structured output.
* **Implementation**: This pattern is **directly implemented** by the `extract_text_between_tags()` function. The LLM **must rely on this function** for all tag-based extraction tasks.
* **When to Use**: For any LLM interaction where the response is structured with tags.

### 2.5 The Second Mind (Critical Reliability Pattern)

* **Intent**: To address common LLM problems such as forgetting details during revisions or failing to fully accomplish a task. The "Second Mind" acts as a reflective, evaluative, and self-correcting architectural layer.
* **Mechanism**: Involves a dedicated process that reviews the primary LLM's output against the original request and expected deliverables.
* **Core Functions**:
    * **Memory Integrity & Gap Detection**: Checks for consistency and completeness.
    * **Output Completeness & Intent Alignment**: Verifies that all parts of the task were completed.
    * **Error Detection & Reasoning Review**: Prompts the LLM to identify potential flaws in its own output.
* **Python Implementation & Application**:
    * The **`hill_climbing()`** function embodies a primary application of the "Second Mind" by incorporating a critique phase where an LLM is asked to act as a "Second Mind" to evaluate and advise on improvements.
    * For specific "Second Mind" checks outside of `hill_climbing`, direct calls to `call_llm()` with specific reflective prompts should be used. The output of such calls will guide further actions.
* **Meta-Principles**:
    * **Redundancy is Strength**: Review beats blind trust.
    * **Introspection Improves Accuracy**: Self-checking leads to higher quality.
    * **Fallibility is the Norm**: Assume the LLM will make mistakes without oversight.

---

## 3. Best Practices for LLM Integration

Adhering to these best practices is **non-negotiable** for all LLM development.

### 3.1 Prompt Design Guidelines

* **Structured Output (Mandatory)**:
    * **ALWAYS** instruct the LLM to enclose its core answer within **`<answer>...</answer>`** tags.
    * **ALWAYS** instruct the LLM to place any additional comments within **`<comments>...</comments>`** tags.
* **Output Format (Mandatory)**:
    * **DO NOT use JSON for output structure**. LLMs frequently misformat JSON. Prefer simple, tag-based formats.
    * **Instruct the LLM to use plain, straight quotes (`"` and `'`)**.
    * **Tell the LLM to avoid emoticons** and other non-standard characters.
    * **Tell the LLM to avoid emdashes (`—`) and en-dashes (`–`)**, using double hyphens (`--`) instead.

### 3.2 Output Handling Guidelines

* **Mandatory Tag Extraction**: **ALWAYS** use **`extract_text_between_tags()`** to reliably retrieve structured content. **DO NOT write custom parsing logic for tags.**
* **Mandatory Text Cleaning**: **ALWAYS** apply **`remove_non_ascii()`** to the extracted text for consistent character formatting.
* **Mandatory Iterative Improvement**: When a task requires reflection or refinement, **ALWAYS** use **`hill_climbing()`**. **Don't implement custom iterative loops.**

### 3.3 Standard LLM Integration Workflow

The overall workflow for interacting with LLMs **must** follow these steps:

1.  **Prepare the Prompt**: Construct your prompt with clear instructions for structured output, **always** specifying the use of `<answer>...</answer>` and `<comments>...</comments>` tags.
2.  **Call the LLM**: Use the **`call_llm()`** function (or its Python equivalent) to send the prompt and receive the response.
3.  **Extract Structured Output**: Immediately use **`extract_text_between_tags()`** to parse the desired content sections.
4.  **Clean Text**: Apply **`remove_non_ascii()`** to the extracted text to ensure clean formatting.
5.  **Iterative Improvement (Conditional)**: If the task requires a refinement loop, employ **`hill_climbing()`** to enhance the quality of the response.

---

## 4. Agentic Programming Pillars (Conceptual Foundations)

These five principles underpin the development of intelligent, self-improving LLM-based systems and guide the application of the Python functions.

1.  **Iterative Improvement (Hill Climbing)**:
    * **Concept**: Continuously refine LLM output by generating candidates and using feedback (`judge_voting()`) to select the "best" version.
    * **Python Implementation**: Directly supported by the **`hill_climbing()`** function.

2.  **Atomic Task Breakdown (Granular Task Planning)**:
    * **Concept**: Decompose complex problems into smaller, manageable subtasks.
    * **Application**: Structure your script to break down large requests into a series of smaller, sequential `call_llm()` operations.

3.  **The Second Mind (Results and Consistency Checking)**:
    * **Concept**: Implement a dedicated reflective layer to review and critique the primary LLM's output.
    * **Python Implementation**: The critique phase within **`hill_climbing()`** directly embodies this pillar. For other checks, use direct calls to `call_llm()` with specific reflective prompts.

4.  **Trusted Transformations (Leveraging LLM Strengths)**:
    * **Concept**: Identify and focus on tasks where LLMs excel (e.g., summarization, translation, code generation, reformatting).
    * **Application**: Design prompts that specifically request these trusted transformations.

5.  **Factual Sourcing and Hallucination Mitigation**:
    * **Concept**: **Never** rely on LLMs as primary sources for factual data. Supply accurate information from trusted external sources (databases, APIs, user input) in your prompts.
    * **Application**: Separate data retrieval (from external sources) from data transformation (by the LLM).

---

## 5. Practical Notes

* **Temporary File Management**: The system **always** uses the `./temp` directory for temporary files for LLM calls. The `create_temp_folder()` and `clear_temp_folder()` functions should be used to manage this directory.
* **Configuration**: Sensitive data (e.g., API keys) **must** be stored in a separate configuration file (e.g., `openrouter_config.txt`). **Never hardcode credentials.**
* **Logging**: LLM calls should generate logs in the `./logs` directory for debugging and monitoring.
* **Output Consistency**: Strict adherence to the tag-based output formats (e.g., `<answer>`, `<comments>`) is crucial for the reliability of the `extract_text_between_tags()` function.
