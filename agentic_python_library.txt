import random
import string

def generate_random_string(length: int = 20) -> str:
    """
    Generates a random alphanumeric string of a given length.

    Args:
        length: The desired length of the random string. Defaults to 20.

    Returns:
        A random string composed of uppercase letters, lowercase letters, and digits.
    """
    # Define the set of characters to choose from.
    # string.ascii_letters includes 'A'-'Z' and 'a'-'z'
    # string.digits includes '0'-'9'
    alphanumeric_chars = string.ascii_letters + string.digits

    # Use random.choices for efficient selection of multiple characters.
    # random.choices returns a list of characters, which we then join into a string.
    random_string = ''.join(random.choices(alphanumeric_chars, k=length))

    return random_string

def trim(input_string: str) -> str:
    """
    Trims leading and trailing whitespace from a string.

    Args:
        input_string: The string to trim.

    Returns:
        The string with leading and trailing whitespace removed.
    """
    return input_string.strip()

import chardet
import codecs # Provides BOM constants for explicit handling

def read_file(file_path: str,
                      fallback_encodings: list[str] = None,
                      chardet_confidence_threshold: float = 0.7,
                      strict_decoding_attempts: bool = True,
                      normalize_line_endings: bool = True) -> str | None:
    """
    Reads a text file with robust encoding detection and error handling,
    returning only the decoded content as a string.

    This function is designed for maximum robustness, blending the strengths of
    automatic detection, controlled fallbacks, and resilient error handling.

    Args:
        file_path (str): The path to the file to read.
        fallback_encodings (list[str], optional): An ordered list of encodings
            to try if chardet's detection isn't confident or if initial strict
            attempts fail. Defaults to a comprehensive list of common encodings.
            It's recommended to include 'utf-8' in this list.
        chardet_confidence_threshold (float, optional): The minimum confidence
            score (0.0-1.0) from 'chardet' required for its detected encoding
            to be used in a strict first attempt. Defaults to 0.7.
        strict_decoding_attempts (bool, optional): If True, the function first
            attempts to decode using 'errors="strict"' with various encodings.
            If all strict attempts fail, it then falls back to 'utf-8' with
            'errors="replace"'. If False, it skips all strict attempts and goes
            directly to 'utf-8' with 'errors="replace"'. Defaults to True.
        normalize_line_endings (bool, optional): If True, converts all line
            endings ('\\r\\n', '\\r') to Unix-style ('\\n'). Defaults to True.

    Returns:
        str: The decoded content of the file.
             Returns an empty string if the file is empty.
             Returns None if the file cannot be opened (e.g., does not exist,
             permission error) or if an unexpected decoding error occurs even
             with 'replace' handling.
    """
    # Initialize default fallback encodings if the user doesn't provide any.
    if fallback_encodings is None:
        fallback_encodings = [
            'utf-8', 'iso-8859-1', 'windows-1252', 'ascii',
            'utf-16-le', 'utf-16-be', 'iso-8859-15',
            'windows-1250', 'windows-1251', 'shift_jis',
            'euc_jp', 'big5'
        ]

    raw_data = None
    try:
        # Step 1: Read the entire file in binary mode.
        with open(file_path, 'rb') as file:
            raw_data = file.read()
            # If the file is empty, no decoding is needed.
            if not raw_data:
                # print(f"Warning: File '{file_path}' is empty.") # Optional: print a warning
                return ""
    except FileNotFoundError:
        # Print an error message for critical issues like file not found.
        print(f"ERROR: File not found: '{file_path}'")
        return None
    except IOError as e:
        # Print an error message for other I/O errors.
        print(f"ERROR: Error reading file '{file_path}' in binary mode: {e}")
        return None

    # Step 2: Use chardet for an initial, probabilistic encoding detection.
    detection = chardet.detect(raw_data)
    detected_encoding = detection.get('encoding')
    confidence = detection.get('confidence', 0.0) or 0.0

    # No print for successful chardet detection unless you add it back.
    # print(f"INFO: Chardet detected encoding: '{detected_encoding}' with confidence: {confidence:.2f}")

    content = None
    
    # Define common Byte Order Marks (BOMs) and their associated standard encoding labels.
    boms = {
        codecs.BOM_UTF8: 'utf-8',
        codecs.BOM_UTF16_LE: 'utf-16-le',
        codecs.BOM_UTF16_BE: 'utf-16-be'
    }

    # Step 3: Build the prioritized list of encodings for strict decoding attempts.
    strict_attempt_encodings = []

    # 3a. Prioritize chardet's guess if its confidence meets the threshold.
    if detected_encoding and confidence >= chardet_confidence_threshold:
        strict_attempt_encodings.append(detected_encoding)
    
    # 3b. Explicitly ensure 'utf-8' is a high priority.
    if 'utf-8' not in strict_attempt_encodings:
        strict_attempt_encodings.insert(0, 'utf-8')
    
    # 3c. Add the remaining user-defined fallback encodings, avoiding duplicates.
    for enc in fallback_encodings:
        if enc not in strict_attempt_encodings:
            strict_attempt_encodings.append(enc)

    # Step 4: Attempt decoding strategies in order of strictness and likelihood.
    if strict_decoding_attempts:
        # print(f"INFO: Attempting strict decoding for '{file_path}' with prioritized encodings: {strict_attempt_encodings}") # Optional: print info
        for current_encoding in strict_attempt_encodings:
            bytes_to_decode_for_attempt = raw_data
            
            # Sub-step 4a: Check for and strip BOMs *before* decoding this attempt.
            for bom_bytes, bom_encoding_label in boms.items():
                if bytes_to_decode_for_attempt.startswith(bom_bytes):
                    norm_current_encoding = current_encoding.lower().replace('-', '_')
                    norm_bom_encoding = bom_encoding_label.lower().replace('-', '_')

                    if norm_bom_encoding == norm_current_encoding:
                        bytes_to_decode_for_attempt = bytes_to_decode_for_attempt[len(bom_bytes):]
                        # print(f"DEBUG: Stripped {bom_encoding_label.upper()} BOM before decoding with '{current_encoding}'.") # Optional: print debug
                    # else:
                        # print(f"DEBUG: File starts with {bom_encoding_label.upper()} BOM, but current attempt is '{current_encoding}'. Not stripping this BOM for this attempt.") # Optional: print debug
                    break 

            try:
                # Sub-step 4b: Attempt strict decode.
                content = bytes_to_decode_for_attempt.decode(current_encoding, errors='strict')
                # print(f"INFO: Successfully decoded with '{current_encoding}' (strict).") # Optional: print info
                break 
            except (UnicodeDecodeError, LookupError) as e:
                # print(f"DEBUG: Strict decode with '{current_encoding}' failed ({e}). Trying next encoding.") # Optional: print debug
                content = None
            except Exception as e:
                # print(f"WARNING: Unexpected error during strict decode with '{current_encoding}': {e}") # Optional: print warning
                content = None

    # Step 5: Final Fallback - If no strict decode succeeded (or strict attempts were skipped).
    if content is None:
        # print(f"WARNING: No strict decoding succeeded for '{file_path}' (or strict attempts were skipped). Falling back to 'utf-8' with 'errors=replace'.") # Optional: print warning
        try:
            bytes_for_final_decode = raw_data
            
            # Explicit BOM stripping for the final 'utf-8' 'replace' attempt.
            for bom_bytes, bom_encoding_label in boms.items():
                if bytes_for_final_decode.startswith(bom_bytes):
                    bytes_for_final_decode = bytes_for_final_decode[len(bom_bytes):]
                    # print(f"DEBUG: Stripped {bom_encoding_label.upper()} BOM for final 'utf-8' replace decode (best effort).") # Optional: print debug
                    break 

            content = bytes_for_final_decode.decode('utf-8', errors='replace')
            # print(f"INFO: Successfully decoded with 'utf-8' (replacing errors).") # Optional: print info
        except Exception as e:
            print(f"ERROR: Failed to decode file '{file_path}' even with 'utf-8' and 'errors=replace': {e}")
            return None

    # Step 6: Normalize line endings if requested.
    if content is not None and normalize_line_endings:
        content = content.replace('\r\n', '\n').replace('\r', '\n')
        # print("DEBUG: Line endings normalized to Unix style.") # Optional: print debug

    return content



def write_file(file_path: str, content: str) -> bool:
    """
    Writes string content to a file in UTF-8 encoding,
    converting Windows-style newlines (CRLF) to Unix-style (LF).

    Args:
        file_path: The path to the file to write.
        content: The string content to write to the file.

    Returns:
        True if the file was written successfully, False otherwise.
    """
    # Convert Windows-style newlines to Unix-style newlines.
    # This matches the Perl regex behavior.
    content = content.replace('\r\n', '\n')

    try:
        # Open the file in write mode ('w') with UTF-8 encoding.
        # 'newline=\n' is crucial: it prevents Python from re-converting
        # '\n' to '\r\n' on Windows systems after we've already normalized.
        # This ensures the file strictly contains Unix-style newlines.
        with open(file_path, 'w', encoding='utf-8', newline='\n') as f:
            f.write(content)
        return True
    except OSError as e:
        # Catch OS-related errors like file not found, permission denied, etc.
        # Print a simple error message to standard error, similar to Perl's warn.
        print(f"Error: Could not write to '{file_path}': {e}", file=__import__('sys').stderr)
        return False

import re
from typing import Optional

def extract_text_between_tags(
    text: str,
    tag: str,
    *,
    strict: bool = False
) -> str:
    """
    Extract the content inside a pair of XML-style tags (<answer>, <comments>, …)
    from an LLM response, coping gracefully with the formatting mistakes LLMs
    often make.

    Parameters
    ----------
    text : str
        The raw LLM response.
    tag : str
        The tag name to extract (case-insensitive, e.g. "answer").
    strict : bool, default False
        • False  – flexible: returns best-effort content even if only one tag
          is present or tags are malformed.  
        • True   – strict: content is returned **only** when a well-formed
          opening/closing pair exists in the correct order.

    Returns
    -------
    str
        Cleaned content from between the tags, or the empty string if nothing
        reliable can be extracted under the chosen strictness.
    """
    lc_tag = tag.lower()
    open_tag = f"<{lc_tag}>"
    close_tag = f"</{lc_tag}>"

    temp = text

    # ------------------------------------------------------------------
    # 1. Normalise tag variants such as "< answer  >" or attributes like
    #    "<answer role=\"assistant\"> …"
    # ------------------------------------------------------------------
    open_re  = re.compile(rf"<\s*{re.escape(lc_tag)}\b[^>]*>",  flags=re.I)
    close_re = re.compile(rf"<\s*/\s*{re.escape(lc_tag)}\b[^>]*>", flags=re.I)
    temp = open_re.sub(open_tag,  temp)
    temp = close_re.sub(close_tag, temp)

    # Extra forgiveness for the common “answer(s)” slip-ups
    if lc_tag == "answer":
        temp = re.sub(r"<\s*/?\s*answers?\b[^>]*>",  lambda m:
                      close_tag if m.group(0).startswith("</") else open_tag,
                      temp, flags=re.I)
        temp = re.sub(r"<\s*/?\s*answe?r\b[^>]*>",  lambda m:
                      close_tag if m.group(0).startswith("</") else open_tag,
                      temp, flags=re.I)

    temp_lc = temp.lower()

    # ------------------------------------------------------------------
    # 2. Special handling when we’re looking for <answer> … </answer>
    # ------------------------------------------------------------------
    if lc_tag == "answer":
        # 2a. Strip any <model> … </model> blocks completely
        temp = re.sub(r"<\s*model\b[^>]*>",  "<model>",  temp, flags=re.I)
        temp = re.sub(r"<\s*/\s*model\b[^>]*>", "</model>", temp, flags=re.I)
        temp  = re.sub(r"<model>.*?</model>", "", temp, flags=re.I | re.S)
        temp_lc = temp.lower()

        # 2b. If </answer> still missing but <comments> or <thinking> follows,
        #     inject the missing close tag before that boundary.
        if close_tag not in temp_lc:
            # Normalise boundary tags first
            temp = re.sub(r"<\s*comments\b[^>]*>", "<comments>", temp, flags=re.I)
            temp = re.sub(r"<\s*thinking\b[^>]*>", "<thinking>", temp, flags=re.I)
            temp_lc = temp.lower()

            pos_comments  = temp_lc.find("<comments>")
            pos_thinking  = temp_lc.find("<thinking>")
            boundary_pos: Optional[int] = None
            if pos_comments != -1 and (pos_thinking == -1 or pos_comments < pos_thinking):
                boundary_pos = pos_comments
            elif pos_thinking != -1:
                boundary_pos = pos_thinking

            if boundary_pos is not None:
                temp = temp[:boundary_pos] + close_tag + temp[boundary_pos:]
                temp_lc = temp.lower()

    # ------------------------------------------------------------------
    # 3. Actual extraction logic
    # ------------------------------------------------------------------
    start = temp_lc.find(open_tag)
    end   = temp_lc.find(close_tag)

    if start != -1 and end != -1 and end > start:
        # Perfect opening/closing pair
        content = temp[start + len(open_tag) : end]

    elif strict:
        return ""                               # Strict mode → reject malformed

    elif start != -1 and end == -1:
        # Only opening tag present (flexible mode)
        content = temp[start + len(open_tag):]
        if lc_tag == "answer":                  # Cut at semantic boundaries
            boundaries = ["<comments>", "<thinking>", "<model>"]
            lower = content.lower()
            stops = [lower.find(b) for b in boundaries if lower.find(b) != -1]
            cut   = min(stops) if stops else len(content)
            content = content[:cut]

    elif start == -1 and end != -1 and not strict:
        # Only closing tag present (flexible mode)
        content = temp[:end]

    else:
        return ""                               # Nothing usable found

    # ------------------------------------------------------------------
    # 4. Cleanup & return
    # ------------------------------------------------------------------
    content = content.strip()
    content = remove_non_ascii(content)
    return content


import re
import unicodedata # For category-based filtering of Unicode characters

def remove_non_ascii(text: str) -> str:
    """
    Removes unwanted Unicode characters—such as a wide range of emoji pictographs,
    smart quotes, em-dashes, zero-width code points, and Unicode control/format
    characters (except line feed, carriage return, and tab)—while preserving
    accented letters, non-Latin scripts, currency symbols, and common typographical
    punctuation. It also normalizes specific punctuation to ASCII equivalents
    and converts all line endings to Unix-style (LF).

    Args:
        text (str): The input string to be cleaned. If not a string, an empty
                    string is returned.

    Returns:
        str: The cleaned and normalized string.
    """

    # 0. Guard against None or non-string inputs.
    # Ensures that subsequent string operations do not raise TypeErrors.
    if not isinstance(text, str):
        return ""

    # 1. Remove a wide range of Emoji and Pictograph characters.
    # This regular expression targets multiple Unicode blocks commonly associated
    # with emojis, symbols, and pictographs. Using re.UNICODE flag ensures
    # the regex correctly interprets these high-codepoint characters.
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # Emoticons
        "\U0001F300-\U0001F5FF"  # Miscellaneous Symbols and Pictographs
        "\U0001F680-\U0001F6FF"  # Transport and Map Symbols
        "\U0001F700-\U0001F77F"  # Alchemical Symbols (extended from other expert's suggestion)
        "\U0001F780-\U0001F7FF"  # Geometric Shapes Extended-A (extended from other expert's suggestion)
        "\U0001F800-\U0001F8FF"  # Supplemental Arrows-C (extended from other expert's suggestion)
        "\U0001F900-\U0001F9FF"  # Supplemental Symbols and Pictographs (extended from other expert's suggestion)
        "\U0001FA00-\U0001FA6F"  # Chess Symbols (extended from other expert's suggestion)
        "\U0001FA70-\U0001FAFF"  # Symbols and Pictographs Extended-B (extended from other expert's suggestion)
        "\u2600-\u26FF"          # Miscellaneous Symbols
        "\u2700-\u27BF"          # Dingbats
        "]+",
        flags=re.UNICODE
    )
    text = emoji_pattern.sub('', text)

    # 2. Normalize common "smart" typographical punctuation to ASCII equivalents.
    # This step ensures consistency across different text sources.

    # 2a. Normalize "smart" double quotes: Left/Right double quotes (U+201C, U+201D)
    # and angle quotes (U+00AB, U+00BB) to a straight double quote (").
    text = re.sub(r'[\u201C\u201D\u00AB\u00BB]', r'"', text)

    # 2b. Normalize "smart" single quotes and apostrophes: Left/Right single quotes
    # (U+2018, U+2019) to a straight single quote (').
    text = re.sub(r'[\u2018\u2019]', r"'", text)

    # 2c. Normalize en-dash (U+2013) and em-dash (U+2014) to two hyphens ("--").
    text = re.sub(r'[\u2013\u2014]', r'--', text)

    # 2d. Normalize horizontal ellipsis (U+2026) to three ASCII dots ("...").
    text = text.replace('\u2026', '...')

    # 3. Remove invisible and non-displaying control/formatting characters.

    # 3a. Remove zero-width characters: These characters control text rendering
    # but are typically invisible and can interfere with parsing or display.
    # U+200B (ZERO WIDTH SPACE)
    # U+200C (ZERO WIDTH NON-JOINER)
    # U+200D (ZERO WIDTH JOINER)
    # U+FEFF (ZERO WIDTH NO-BREAK SPACE) - also used as BOM, but here in middle of text
    text = re.sub(r'[\u200B\u200C\u200D\uFEFF]', r'', text)

    # 3b. Remove Variation Selectors (U+FE0E, U+FE0F).
    # These characters determine text or emoji presentation style and are usually
    # not needed in plain text content.
    text = re.sub(r'[\uFE0E\uFE0F]', r'', text)

    # 4. Remove "Other" category Unicode characters (\p{C} equivalent in Perl).
    # This targets characters in the Unicode "Other" category (Control, Format,
    # Unassigned, Private Use, Surrogates), which are generally unwanted in clean text.
    # We explicitly preserve essential whitespace control characters: line feed (\n),
    # carriage return (\r), and tab (\t).
    cleaned_chars = []
    for char in text:
        # Preserve fundamental structural whitespace characters.
        if char == '\r' or char == '\n' or char == '\t':
            cleaned_chars.append(char)
        else:
            # Check the Unicode General Category of the character.
            # Categories starting with 'C' are "Other" (Cc, Cf, Cn, Co, Cs).
            # Characters not in the 'C' category are kept. This ensures accented
            # characters, non-Latin scripts, currency symbols, and most punctuation
            # and symbols are preserved.
            if not unicodedata.category(char).startswith('C'):
                cleaned_chars.append(char)
            # Otherwise, the character is an unwanted 'C' category character and is filtered out.

    text = "".join(cleaned_chars)

    # 5. Normalize line endings: convert CRLF (\r\n) and CR (\r) to LF (\n).
    # Ensures consistent line breaks regardless of the originating operating system.
    text = text.replace('\r\n', '\n').replace('\r', '\n')

    return text

import os

def create_temp_folder(folder_path: str = "./temp") -> None:
    """
    Creates a temporary folder if it does not already exist.

    Args:
        folder_path: The path to the folder to create. Defaults to "./temp".
    """
    try:
        # Use os.makedirs with exist_ok=True to create the directory
        # if it doesn't exist. If it does exist, nothing happens.
        # This also creates any necessary parent directories.
        os.makedirs(folder_path, exist_ok=True)
    except OSError as e:
        # Catch OSError for potential issues like permission errors
        # or invalid path names.
        print(f"Error: Failed to create temp folder '{folder_path}': {e}",
              file=__import__('sys').stderr)
        # In Python, functions typically raise exceptions for unrecoverable errors.
        # Since the Perl original 'die's, we'll re-raise here after printing.
        raise

import time
import sys
from pathlib import Path # Explicitly import Path

def clear_temp_folder(threshold: float = 1.0, folder: str | Path = "./temp") -> None:
    """
    Delete files older than `threshold` seconds from `folder`.

    Parameters
    ----------
    threshold : float
        Age cutoff in seconds. Non‐positive values default to 1 second.
    folder : str or Path
        Directory to clean. Defaults to "./temp".
    """
    # Normalize inputs
    if threshold <= 0:
        threshold = 1.0
    folder_path = Path(folder)

    # Nothing to do if the directory doesn’t exist or isn't a directory
    # This also implicitly handles permission errors for checking existence/type
    if not folder_path.is_dir():
        # Optional: You could add a warning here if you expect the folder to exist
        # print(f"Warning: Folder '{folder_path}' does not exist or is not a directory. Skipping clear.", file=sys.stderr)
        return

    now = time.time()

    for entry in folder_path.iterdir():
        # Skip subdirectories and non-files
        # is_file() checks if it's a regular file (not a dir, symlink, etc.)
        if not entry.is_file():
            continue

        # Obtain inode change time (ctime)
        # Use a try-except here because stat() can fail for various reasons (e.g., broken symlink)
        try:
            ctime = entry.stat().st_ctime
        except OSError as e:
            # Print a warning but continue processing other files
            print(f"Warning: cannot get status for '{entry!s}': {e}", file=sys.stderr)
            continue

        # If older than threshold, delete
        if (now - ctime) > threshold:
            try:
                # Uncomment the next line to log deletions:
                # print(f"Deleting old file: {entry}")
                entry.unlink() # Deletes the file
            except OSError as e:
                # Print a warning but continue processing other files
                print(f"Warning: could not delete '{entry!s}': {e}", file=sys.stderr)

from pathlib import Path
import os # Still needed for os.sep if you explicitly rstrip by it, or for os.PathLike if using that type hint

def ensure_directory(directory_path: str | Path) -> None:
    """
    Ensures that the given directory exists, creating any intermediate
    directories as needed.

    Args:
        directory_path: The path to the directory to ensure exists.
                        Can be a string or a pathlib.Path object.

    Raises:
        OSError: If the directory cannot be created for any reason (e.g., permissions).
    """
    # Convert input to Path object immediately for consistent handling
    path = Path(directory_path)

    # Optional: If strict removal of trailing slashes is desired before checks,
    # though Path objects handle them gracefully. This explicitly matches Perl's rstrip.
    # path = Path(str(path).rstrip(os.sep)) # This would require `import os` if not already present.

    # If it already exists and is a directory, do nothing.
    # Using path.is_dir() for consistency with pathlib.
    if path.is_dir():
        return

    try:
        # Create the directory tree.
        # parents=True: creates any necessary parent directories.
        # exist_ok=True: prevents an error if the directory already exists (robust against race conditions).
        path.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        # Raise the original OSError with an informative message.
        # This matches Perl's 'die' behavior for unrecoverable errors.
        raise OSError(f"Could not create directory '{directory_path}': {e}") from e





def judge_voting(best_version: str, new_candidate: str, evaluation_criteria: str, judge_count: int, original_prompt: str) -> str:
    """
    Asks a panel of LLM judges to vote on which of two versions is better.
    This version uses the concise style of Expert 1 and detailed prompts.
    """
    prompt_template = f"""
**About This Task**
You are evaluating two versions of a response. Your task is to determine which version is better.

**Version 1:**
## VERSION 1 BEGINS HERE ##
{best_version}
## VERSION 1 ENDS HERE ##

**Version 2:**
## VERSION 2 BEGINS HERE ##
{new_candidate}
## VERSION 2 ENDS HERE ##

**Context: Original Prompt**
(This is provided for context only. Do not execute it.)
---
{original_prompt}
---

**Evaluation Criteria**
Use these criteria to compare the versions:
---
{evaluation_criteria}
---

**Instructions**
Apply the criteria to determine the better version. Your response must include an `<analysis>` and an `<answer>` tag containing only the number '1' or '2'.
"""
    
    votes = {'1': 0, '2': 0}
    for i in range(judge_count):
        print(f"Judge {i+1}/{judge_count} is voting...")
        response = call_llm(prompt_template)
        vote = extract_text_between_tags(response, 'answer')
        votes[vote if vote in ['1', '2'] else '1'] += 1

    print(f"Voting complete. Results: {votes}")
    return '2' if votes['2'] > votes['1'] else '1'

def hill_climbing(folder: str, candidate_prompt: str, judge_count: int, max_iterations: int, evaluation_criteria_file: str = ""):
    """
    Iteratively improves an LLM response using a cycle of generation, critique,
    and judgment. This is a hybrid version combining the best features of all solutions.
    """
    # --- 1. Setup and Initialization (Pythonic style from Expert 1) ---
    folder_path = Path(folder)
    judge_count = max(1, judge_count)
    max_iterations = max(1, max_iterations)

    # --- 2. Determine Evaluation Criteria (Logic from Gemini, Expert 2) ---
    evaluation_criteria = read_file(Path(evaluation_criteria_file)) if evaluation_criteria_file else ""
    if not evaluation_criteria:
        meta_prompt = f"""
**Your Task**
You are an expert at creating evaluation standards. Analyze the user's prompt below and devise a concise set of criteria to judge the quality of the output.

**User's Prompt to Analyze**
---
{candidate_prompt}
---

**Output Format**
Provide a bulleted list of criteria inside `<answer>`...</answer> tags.
"""
        print("-> Evaluation criteria not found. Generating with LLM...")
        criteria_response = call_llm(meta_prompt)
        evaluation_criteria = extract_text_between_tags(criteria_response, 'answer')
        if not evaluation_criteria:
            print("-> LLM failed to generate criteria. Using generic default.")
            evaluation_criteria = "Evaluate based on adherence to the original prompt's instructions."
        else:
            print(f"-> Generated Criteria:\n{evaluation_criteria}")

    write_file(folder_path / "evaluation_criteria.txt", evaluation_criteria)
    
    # --- 3. Generate Initial Candidate ---
    print("\n--- Generating Initial Candidate ---")
    response = call_llm(candidate_prompt)
    initial_solution = extract_text_between_tags(response, 'answer')
    write_file(folder_path / "best.txt", initial_solution)
    print("Initial candidate saved to best.txt")

    # --- 4. Iterative Refinement Loop (with `initial_probes` from Expert 2) ---
    initial_probes = min(max(int(max_iterations * 0.1) + 1, 1), 5)
    
    for i in range(1, max_iterations):
        print(f"\n--- Hill-Climbing Iteration: {i+1}/{max_iterations} ---")
        
        # REFINED: Read the definitive 'best' solution at the start of each loop
        best_solution = read_file(folder_path / "best.txt")
        
        # (A) Critique Phase
        print("(A) Critiquing current best solution...")
        critique_prompt = f"""
**Your Task**
Act as a critic. Review the solution below based on the original prompt and evaluation criteria. Provide concrete advice for improvement.

**Original Prompt:**
---
{candidate_prompt}
---
**Current Best Solution:**
---
{best_solution}
---
**Evaluation Criteria:**
---
{evaluation_criteria}
---

**Assignment**
Provide constructive recommendations inside `<answer>`...</answer> tags.
"""
        response = call_llm(critique_prompt)
        advice = extract_text_between_tags(response, 'answer') or "No specific advice given."
        print(f"Advice received: {advice}")

        # (B) New Candidate Generation (using `.replace()` from Expert 1 & 2)
        print("(B) Generating new candidate with advice...")
        new_candidate_prompt = candidate_prompt

        # Conditionally add the previous solution after initial probes
        if i >= initial_probes and '{previous_solution}' in new_candidate_prompt:
             new_candidate_prompt = new_candidate_prompt.replace('{previous_solution}', best_solution)
        
        # Append advice for improvement
        new_candidate_prompt += f"""

**Advice for Improvement**
An expert provided the following advice. Use it to create an improved version.
---
{advice}
---
"""
        response = call_llm(new_candidate_prompt)
        candidate = extract_text_between_tags(response, 'answer')
        write_file(folder_path / "candidate.txt", candidate)

        # (C) Judgment Phase
        print("(C) Judging: Comparing best vs. new candidate...")
        judgement = judge_voting(best_solution, candidate, evaluation_criteria, judge_count, candidate_prompt)

        # (D) Promotion/Update Phase
        print(f"(D) Judgement result: {'New candidate wins!' if judgement == '2' else 'Current best remains.'}")
        if judgement == '2' and candidate:
            print("Promoting new candidate to best.txt")
            write_file(folder_path / "best.txt", candidate)
            # The 'best_solution' variable will be updated by the read_file at the top of the next loop

    print("\n--- Hill-Climbing Process Complete ---")
    print(f"The final best solution is in {folder_path / 'best.txt'}")

if __name__ == '__main__':
    # --- Example Usage (Runnable structure from Gemini solution) ---
    output_folder = "hill_climb_hybrid_output"
    
    # Use a prompt with a placeholder for the `.replace()` method
    example_prompt_template = """
Explain the concept of "hill climbing" in the context of AI optimization.
Keep the explanation to a single paragraph.

For context, here is the previous attempt (if any):
{previous_solution}
"""
    
    hill_climbing(
        folder=output_folder,
        candidate_prompt=example_prompt_template.replace('{previous_solution}', 'There is no previous solution.'),
        judge_count=3,
        max_iterations=3,
        evaluation_criteria_file=""
    )

    final_result_path = Path(output_folder) / "best.txt"
    if final_result_path.exists():
        print(f"\nFinal optimized result:\n---\n{read_file(final_result_path)}")

